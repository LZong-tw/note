## 分而治之

### 方法介紹

對於海量數據而言，由於無法一次性裝進內存處理，導致我們不得不把海量的數據通過hash映射分割成相應的小塊數據，然後再針對各個小塊數據通過hash_map進行統計或其它操作。

那什麼是hash映射呢？簡單來說，就是為了便於計算機在有限的內存中處理big數據，我們通過一種映射散列的方式讓數據均勻分佈在對應的內存位置(如大數據通過取餘的方式映射成小數存放在內存中，或大文件映射成多個小文件)，而這個映射散列方式便是我們通常所說的hash函數，設計的好的hash函數能讓數據均勻分佈而減少衝突。

### 問題實例

**1、海量日誌數據，提取出某日訪問百度次數最多的那個IP**

**分析**：百度作為國內第一大搜索引擎，每天訪問它的IP數量巨大，如果想一次性把所有IP數據裝進內存處理，則內存容量明顯不夠，故針對數據太大，內存受限的情況，可以把大文件轉化成（取模映射）小文件，從而大而化小，逐個處理。

換言之，先映射，而後統計，最後排序。

**解法**：具體分為以下3個步驟
 - 1.分而治之/hash映射
- 首先把這一天訪問百度日誌的所有IP提取出來，然後逐個寫入到一個大文件中，接著採用映射的方法，比如%1000，把整個大文件映射為1000個小文件。
 - 2.hash_map統計
- 當大文件轉化成了小文件，那麼我們便可以採用hash_map(ip, value)來分別對1000個小文件中的IP進行頻率統計，再找出每個小文件中出現頻率最大的IP。
 - 3.堆/快速排序
- 統計出1000個頻率最大的IP後，依據各自頻率的大小進行排序(可採取堆排序)，找出那個頻率最大的IP，即為所求。

**注**：Hash取模是一種等價映射，不會存在同一個元素分散到不同小文件中去的情況，即這裡採用的是%1000算法，那麼同一個IP在hash後，只可能落在同一個文件中，不可能被分散的。

**2、尋找熱門查詢，300萬個查詢字符串中統計最熱門的10個查詢**

**原題**：搜索引擎會通過日誌文件把用戶每次檢索使用的所有檢索串都記錄下來，每個查詢串的長度為1-255字節。假設目前有一千萬個記錄，請你統計最熱門的10個查詢串，要求使用的內存不能超過1G。

**分析**：這些查詢串的重複度比較高，雖然總數是1千萬，但如果除去重複後，不超過3百萬個。一個查詢串的重複度越高，說明查詢它的用戶越多，也就是越熱門。

由上面第1題，我們知道，數據大則劃為小的，例如一億個ip求Top 10，可先%1000將ip分到1000個小文件中去，並保證一種ip只出現在一個文件中，再對每個小文件中的ip進行hash_map統計並按數量排序，最後歸併或者最小堆依次處理每個小文件的top10以得到最後的結果。

但對於本題，數據規模比較小，能一次性裝入內存。因為根據題目描述，雖然有一千萬個Query，但是由於重複度比較高，故去除重複後，事實上只有300萬的Query，每個Query255Byte，因此我們可以考慮把他們都放進內存中去（300萬個字符串假設沒有重複，都是最大長度，那麼最多佔用內存3M*1K/4=0.75G。所以可以將所有字符串都存放在內存中進行處理）。

所以我們放棄分而治之/hash映射的步驟，直接上hash_map統計，然後排序。So，針對此類典型的TOP K問題，採取的對策往往是：hash_map + 堆。

**解法**：

 - 1.hash_map統計
- 先對這批海量數據預處理。具體方法是：維護一個Key為Query字串，Value為該Query出現次數的hash_map，即hash_map(Query, Value)，每次讀取一個Query，如果該字串不在Table中，那麼加入該字串，並將Value值設為1；如果該字串在Table中，那麼將該字串的計數加1 即可。最終我們在O(N)的時間複雜度內用hash_map完成了統計；
 - 2.堆排序
- 藉助堆這個數據結構，找出Top K，時間複雜度為N‘logK。即藉助堆結構，我們可以在log量級的時間內查找和調整/移動。因此，維護一個K(該題目中是10)大小的小根堆，然後遍歷300萬的Query，分別和根元素進行對比。所以，我們最終的時間複雜度是：O(n) + N' * O(logk），其中，N為1000萬，N’為300萬。

關於第2步堆排序，可以維護k個元素的最小堆，即用容量為k的最小堆存儲最先遍歷到的k個數，並假設它們即是最大的k個數，建堆費時O（k），並調整堆(費時O(logk))後，有k1>k2>...kmin（kmin設為小頂堆中最小元素）。繼續遍歷數列，每次遍歷一個元素x，與堆頂元素比較，若x>kmin，則更新堆（x入堆，用時logk），否則不更新堆。這樣下來，總費時O（k*logk+（n-k）*logk）=O（n*logk）。此方法得益於在堆中，查找等各項操作時間複雜度均為logk。

當然，你也可以採用trie樹，關鍵字域存該查詢串出現的次數，沒有出現為0。最後用10個元素的最小推來對出現頻率進行排序。

**3、有一個1G大小的一個文件，裡面每一行是一個詞，詞的大小不超過16字節，內存限制大小是1M。返回頻數最高的100個詞**

**解法**：

 - 1.分而治之/hash映射
- 順序讀取文件，對於每個詞x，取hash(x)%5000，然後把該值存到5000個小文件（記為x0,x1,...x4999）中。這樣每個文件大概是200k左右。當然，如果其中有的小文件超過了1M大小，還可以按照類似的方法繼續往下分，直到分解得到的小文件的大小都不超過1M。
 - 2.hash_map統計
- 對每個小文件，採用trie樹/hash_map等統計每個文件中出現的詞以及相應的頻率。
 - 3.堆/歸併排序
- 取出出現頻率最大的100個詞（可以用含100個結點的最小堆）後，再把100個詞及相應的頻率存入文件，這樣又得到了5000個文件。最後就是把這5000個文件進行歸併（類似於歸併排序）的過程了。

**4、海量數據分佈在100臺電腦中，想個辦法高效統計出這批數據的TOP10**

**解法一**：

如果同一個數據元素只出現在某一臺機器中，那麼可以採取以下步驟統計出現次數TOP10的數據元素：

 - 1.堆排序
- 在每臺電腦上求出TOP 10，可以採用包含10個元素的堆完成（TOP 10小，用最大堆，TOP 10大，用最小堆，比如求TOP10大，我們首先取前10個元素調整成最小堆，如果發現，然後掃描後面的數據，並與堆頂元素比較，如果比堆頂元素大，那麼用該元素替換堆頂，然後再調整為最小堆。最後堆中的元素就是TOP 10大）。
 - 2.組合歸併
- 求出每臺電腦上的TOP 10後，然後把這100臺電腦上的TOP 10組合起來，共1000個數據，再利用上面類似的方法求出TOP 10就可以了。

**解法二**：

但如果同一個元素重複出現在不同的電腦中呢，比如拿兩臺機器求top 2的情況來說：

 - 第一臺的數據分佈及各自出現頻率為：a(50)，b(50)，c(49)，d(49) ，e(0)，f(0)
- 其中，括號裡的數字代表某個數據出現的頻率，如a(50)表示a出現了50次。 
 - 第二臺的數據分佈及各自出現頻率為：a(0)，b(0)，c(49)，d(49)，e(50)，f(50)

這個時候，你可以有兩種方法：

* 遍歷一遍所有數據，重新hash取摸，如此使得同一個元素只出現在單獨的一臺電腦中，然後採用上面所說的方法，統計每臺電腦中各個元素的出現次數找出TOP 10，繼而組合100臺電腦上的TOP 10，找出最終的TOP 10。
* 或者，暴力求解：直接統計統計每臺電腦中各個元素的出現次數，然後把同一個元素在不同機器中的出現次數相加，最終從所有數據中找出TOP 10。

**5、有10個文件，每個文件1G，每個文件的每一行存放的都是用戶的query，每個文件的query都可能重複。要求你按照query的頻度排序**

**解法一**：

 - 1.hash映射
- 順序讀取10個文件，按照hash(query)%10的結果將query寫入到另外10個文件（記為a0,a1,..a9）中。這樣新生成的文件每個的大小大約也1G（假設hash函數是隨機的）。
 - 2.hash_map統計
- 找一臺內存在2G左右的機器，依次對用hash_map(query, query_count)來統計每個query出現的次數。注：hash_map(query, query_count)是用來統計每個query的出現次數，不是存儲他們的值，出現一次，則count+1。
 - 3.堆/快速/歸併排序
- 利用快速/堆/歸併排序按照出現次數進行排序，將排序好的query和對應的query_cout輸出到文件中，這樣得到了10個排好序的文件（記為![](../images/8/8.1/8.1.2.gif)）。最後，對這10個文件進行歸併排序（內排序與外排序相結合）。

**解法二**：

一般query的總量是有限的，只是重複的次數比較多而已，可能對於所有的query，一次性就可以加入到內存了。這樣，我們就可以採用trie樹/hash_map等直接來統計每個query出現的次數，然後按出現次數做快速/堆/歸併排序就可以了。

**解法三**：

與解法1類似，但在做完hash，分成多個文件後，可以交給多個文件來處理，採用分佈式的架構來處理（比如MapReduce），最後再進行合併。

**6、給定a、b兩個文件，各存放50億個url，每個url各佔64字節，內存限制是4G，讓你找出a、b文件共同的url？**

**解法**：

可以估計每個文件安的大小為5G×64=320G，遠遠大於內存限制的4G。所以不可能將其完全加載到內存中處理。考慮採取分而治之的方法。

 - 1.分而治之/hash映射
- 遍歷文件a，對每個url求取![](../images/8/8.1/8.1.3.gif)，然後根據所取得的值將url分別存儲到1000個小文件（記為![](../images/8/8.1/8.1.4.gif)，這裡漏寫個了a1）中。這樣每個小文件的大約為300M。遍歷文件b，採取和a相同的方式將url分別存儲到1000小文件中（記為![](../images/8/8.1/8.1.5.gif)）。這樣處理後，所有可能相同的url都在對應的小文件（![](../images/8/8.1/8.1.6.gif)）中，不對應的小文件不可能有相同的url。然後我們只要求出1000對小文件中相同的url即可。
 - 2.hash_set統計
- 求每對小文件中相同的url時，可以把其中一個小文件的url存儲到hash_set中。然後遍歷另一個小文件的每個url，看其是否在剛才構建的hash_set中，如果是，那麼就是共同的url，存到文件裡面就可以了。

**7、100萬個數中找出最大的100個數**

**解法一**：採用局部淘汰法。選取前100個元素，並排序，記為序列L。然後一次掃描剩餘的元素x，與排好序的100個元素中最小的元素比，如果比這個最小的要大，那麼把這個最小的元素刪除，並把x利用插入排序的思想，插入到序列L中。依次循環，知道掃描了所有的元素。複雜度為O(100萬*100)。

**解法二**：採用快速排序的思想，每次分割之後只考慮比主元大的一部分，直到比主元大的一部分比100多的時候，採用傳統排序算法排序，取前100個。複雜度為O(100萬*100)。

**解法三**：在前面的題中，我們已經提到了，用一個含100個元素的最小堆完成。複雜度為O(100萬*lg100)。


### 舉一反三

**1**、怎麼在海量數據中找出重複次數最多的一個？

提示：先做hash，然後求模映射為小文件，求出每個小文件中重複次數最多的一個，並記錄重複次數。然後找出上一步求出的數據中重複次數最多的一個就是所求（具體參考前面的題）。

**2**、上千萬或上億數據（有重複），統計其中出現次數最多的前N個數據。

提示：上千萬或上億的數據，現在的機器的內存應該能存下。所以考慮採用hash_map/搜索二叉樹/紅黑樹等來進行統計次數。然後就是取出前N個出現次數最多的數據了，可以用第2題提到的堆機制完成。

**3**、一個文本文件，大約有一萬行，每行一個詞，要求統計出其中最頻繁出現的前10個詞，請給出思想，給出時間複雜度分析。

提示：這題是考慮時間效率。用trie樹統計每個詞出現的次數，時間複雜度是O(n\*le)（le表示單詞的平準長度）。然後是找出出現最頻繁的前10個詞，可以用堆來實現，前面的題中已經講到了，時間複雜度是O(n\*lg10)。所以總的時間複雜度，是O(n\*le)與O(n\*lg10)中較大的哪一個。

**4**、1000萬字符串，其中有些是重複的，需要把重複的全部去掉，保留沒有重複的字符串。請怎麼設計和實現？

提示：這題用trie樹比較合適，hash_map也行。當然，也可以先hash成小文件分開處理再綜合。

**5**、一個文本文件，找出前10個經常出現的詞，但這次文件比較長，說是上億行或十億行，總之無法一次讀入內存，問最優解。

提示：首先根據用hash並求模，將文件分解為多個小文件，對於單個文件利用上題的方法求出每個文件件中10個最常出現的詞。然後再進行歸併處理，找出最終的10個最常出現的詞。
