---
title: 時序數據庫簡介
layout: post
comments: true
language: chinese
category: [database]
keywords:
description:
---


<!-- more -->

## 簡介

例如，你在負責運維一個三方的支付平臺，整個系統大致分為了 交易、支付、帳務 三個系統。

假設今天凌晨兩點時有用戶保障，說無法創建交易，首先判斷下是否是由於負載過大引起的，那麼我們需要查詢 "今天兩點交易服務器的負載超過15的機器有那些"，對於這樣的多緯度分組聚合查詢，時序數據庫是一個很好的選擇。

### 時序數據

時序數據以時間作為主要的查詢緯度，通常會將連續的多個時間序列數據繪製成線，可製作基於時間的多緯度報表，用揭示其趨勢、規律、異常，除了實時在線預測和預警，還可以做離線的數據分析甚至是機器學習。

時序數據庫就是存放時序數據的數據庫，並且需要支持時序數據的快速寫入、持久化、多緯度的聚合查詢等基本功能。

### 基本概念

不同的時序數據庫定義可能會有所不同，如下是其使用時常見的基本概念。

* metric 度量，相當於關係型數據庫中的 table；
* data point 數據點，相當於關係型數據庫中的 row；
* timestamp 時間戳，代表數據點產生的時間，一般是必須的列，常作為主健；
* field 度量下的不同字段，一般存放隨著時間戳的變化而變化的數據；
* tag 標籤，一般存放的是標示數據點來源的屬性信息。

通常 timestamp 加上所有的 tags 可以認為是 table 的 key；如下圖是採集風向的數據：

![timeseries metric wind]({{ site.url }}/images/databases/timeseries-metric-wind.jpg "timeseries metric wind"){: .pull-center }

### 難點

時序數據庫需要解決以下幾個核心的問題：

* 時序數據的寫入，支持每秒鐘上千萬上億數據點的寫入；
* 時序數據的讀取，支持在秒級對上億數據的分組聚合運算；
* 存儲成本，如何保存大量的數據，降低存儲成本；

## 存儲方案

一般分為了單機存儲以及分佈式存儲。

### 單機存儲

如果只存儲數據，直接寫日誌即可，但因為需要快速聚合查詢，所以需要考慮存儲的結構。

傳統數據庫存儲採用的都是 B-Tree，主要用於在查詢和順序插入時有利於減少尋道次數，對於普通機械磁盤，一般尋道時間大概需要 10ms 左右，對於隨機讀寫大部分時間會消耗在尋道上，從而導致整個查詢非常慢。

雖然 SSD 具有更快的尋道時間，但並沒有從根本上解決這個問題，而且會引入新的問題。

對於 90% 以上場景都是寫入的時序數據庫，B Tree 很明顯是不合適，大部分採用 LSM Tree 替換，如 HBase、Cassandra 等；LSM Tree 包括內存裡的數據結構和磁盤上的文件兩部分，分別對應 HBase 裡的 MemStore/HLog 以及 Cassandra 裡的 MemTable/SSTable 。

LSM Tree 的操作流程如下：

1. 數據寫入和更新先寫入內存裡的數據結構，為了避免數據丟失同時也會先寫到 WAL 文件中。
2. 內存裡的數據會定時或者當達到固定大小時刷到磁盤，這些磁盤上的文件不會被修改。
3. 隨著磁盤上積累的文件越來越多，會定時的進行合併操作，消除冗餘數據，減少文件數量。

![timeseries lsmtree arch]({{ site.url }}/images/databases/timeseries-lsmtree-arch.png "timeseries lsmtree arch"){: .pull-center }

可以看到 LSM Tree 的核心思想就是通過內存寫和後續磁盤的順序寫入獲得更高的寫入性能，避免了隨機寫入，但同時也犧牲了讀取性能，因為同一個 key 的值可能存在於多個 HFile 中。

為了獲取更好的讀取性能，可以通過 BloomFilter 和 Compaction 機制。


### 分佈式存儲

時序數據庫面向的是海量數據的寫入存儲讀取，單機是無法解決問題的，所以需要採用多機存儲，也就是分佈式存儲；除了數據量的問題之外，通常也可以通過分佈式解決單點問題。

對於分佈式存儲首先需要考慮如何將數據分佈到多臺機器上面，目前採用最多的是分片，也可以通過單獨結點管理數據分片；關於分片，其核心問題是分片方法的選擇和分片的設計。

#### 分片方法

時序數據庫的分片方法和其他分佈式系統是相通的，基本分為如下幾種：

* 哈希分片，實現簡單，均衡性較好，但是集群不易擴展，動態增刪結點容易導致大部分數據重新分佈。
* 一致性哈希，這種方案均衡性好，集群擴展容易，只是實現相比略微複雜，例如 DynamoDB(Amazon)、Cassandra 。
* 範圍劃分，通常配合全局有序，複雜度在於合併和分裂，例如 Hbase 。

#### 分片設計

所謂的分片設計，簡單來說就是通過什麼計算分片，這是非常有技巧的，將會直接影響讀寫性能。

結合時序數據的特點，通常會根據 metric+tags 進行分片，因為往往會按照一個時間範圍查詢，這樣相同 metric 和 tags 的數據會在一臺機器上連續存放，順序的磁盤讀取是很快的。

<!--
進一步我們考慮時序數據時間範圍很長的情況，需要根據時間範圍再將分成幾段，分別存儲到不同的機器上，這樣對於大範圍時序數據就可以支持併發查詢，優化查詢速度。
-->

如下圖，第一行和第三行都是同樣的 tag(sensor=95D8-7913;city=上海)，所以分配到同樣的分片，而第五行雖然也是同樣的 tag，但是根據時間範圍再分段，被分到了不同的分片；而第二、四、六行與上述相同。

![timeseries metric sharding examples]({{ site.url }}/images/databases/timeseries-metric-sharding-examples.jpg "timeseries metric sharding examples"){: .pull-center }

<!--
## 產品解析

### InfluxDB

非常優秀的時序數據庫，但只有單機版是免費開源的，集群版本是要收費的。從單機版本中可以一窺其存儲方案：在單機上InfluxDB採取類似於LSM tree的存儲結構TSM;而分片的方案InfluxDB先通過+(事實上還要加上retentionPolicy)確定ShardGroup，再通過+的hash code確定到具體的Shard。

　　這裡timestamp默認情況下是7天對齊，也就是說7天的時序數據會在一個Shard中。

timeseries-influxdb-arch.png
-->

## 數據查詢

對於時序數據的查詢分為兩種：原始數據的查詢和時序數據聚合運算的查詢，前者主要對歷史高精度時序數據的查詢，查詢結果粒度太細，並不利於分析其規律或者趨勢，也不適合展現給用戶；後者主要用來對數據做分析。

<!--
例如dashboard等UI工具使用聚合查詢展示數據分析結果。通常數據分析的查詢範圍廣，查詢的數據量大，從而導致查詢的延時比較高，而往往分析工具又要求查詢延時低，大數據量低延時是時序數據查詢面臨的主要問題，本文主要探討聚合分析查詢的優化。

　　2. 時序數據的查詢的優化

　　從前文可瞭解到，時序數據的存儲主要包含單機和分佈式存儲。時序數據根據分片規則(通常使用metric+tags+時間範圍)，將分片存儲在單機或者分佈式環境中。聚合運算查詢時，根據查詢條件查詢所有的數據分片，所有的分片按照時間戳合併形成原始數據結果，當查詢條件包含聚合運算時，會根據採樣窗口對數據進行聚合運算，最後返回運算結果。

　　數據聚合運算查詢延時的計算可以粗略的描述如下：

　　聚合運算查詢：數據分片的查詢合併 + 聚合運算+ 數據返回



-->

## 參考

<!--
http://www.sohu.com/a/153028048_115080
https://www.csdn.net/article/a/2017-05-18/15927958
https://yq.aliyun.com/articles/162566


https://zhuanlan.zhihu.com/p/22651783


http://blog.jobbole.com/88475/
http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/
-->

{% highlight text %}
{% endhighlight %}

